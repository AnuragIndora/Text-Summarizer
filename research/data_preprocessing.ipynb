{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05770c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import json \n",
    "import re \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96eccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5604924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd \n",
    "\n",
    "folder_path = \"C:\\\\Users\\\\anura\\\\OneDrive\\\\Documents\\\\GitHub\\\\Text-Summarizer\\\\data\\\\\"\n",
    "\n",
    "train_path = folder_path + \"train.csv\"\n",
    "validation_path = folder_path + \"validation.csv\"\n",
    "test_path = folder_path + \"test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "validation_df = pd.read_csv(validation_path)\n",
    "test_df = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e91c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape, validation_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34be06bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = [train_df, validation_df, test_df]\n",
    "\n",
    "for df in ds:\n",
    "    print(df.sample(1))\n",
    "    print(\"--\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d02cc5",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed65017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "from typing import Dict, List\n",
    "\n",
    "class SummarizationPreprocessor:\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer,\n",
    "                 max_input_length: int = 1024,\n",
    "                 max_target_length: int = 128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __call__(self, batch: Dict[str, List[str]]) -> Dict[str, List[int]]:\n",
    "        # Tokenize the article (input text)\n",
    "        inputs = self.tokenizer(\n",
    "            batch[\"article\"],\n",
    "            max_length=self.max_input_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "        # Tokenize the highlights (target summary)\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            labels = self.tokenizer(\n",
    "                batch[\"highlights\"],\n",
    "                max_length=self.max_target_length,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "\n",
    "        # Add labels to the inputs\n",
    "        inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11012771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BartTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "# Load your CSVs\n",
    "train_dataset = load_dataset(\"csv\", data_files=train_path)[\"train\"]\n",
    "val_dataset = load_dataset(\"csv\", data_files=validation_path)[\"train\"]\n",
    "test_dataset = load_dataset(\"csv\", data_files=test_path)[\"train\"]\n",
    "\n",
    "# Create preprocessor\n",
    "preprocessor = SummarizationPreprocessor(tokenizer)\n",
    "\n",
    "# Apply it using map with batching\n",
    "tokenized_train = train_dataset.map(preprocessor, batched=True, remove_columns=[\"article\", \"highlights\"])\n",
    "tokenized_val = val_dataset.map(preprocessor, batched=True, remove_columns=[\"article\", \"highlights\"])\n",
    "tokenized_test = test_dataset.map(preprocessor, batched=True, remove_columns=[\"article\", \"highlights\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f85f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"training dataset: {tokenized_train}\")\n",
    "print(f\"validation dataset: {tokenized_val}\")\n",
    "print(f\"test dataset: {tokenized_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5aa826",
   "metadata": {},
   "source": [
    "# model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d90fe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d7fa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f372ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc30db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade \"accelerate>=0.26.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6ebc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip show accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c79121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bb20e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    logging_steps=500,\n",
    "    save_steps=1000,\n",
    "    per_device_train_batch_size=2,        # Small batch for limited GPU\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,        # Effective larger batch\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,           # âœ… Needed for summarization\n",
    "    fp16=True,                            # Use if you have a compatible GPU\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",                     # Can be \"wandb\", \"tensorboard\", etc.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd216dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,                    # e.g. BartForConditionalGeneration or T5ForConditionalGeneration\n",
    "    args=training_args,             # instance of Seq2SeqTrainingArguments\n",
    "    train_dataset=tokenized_train, # your pre-tokenized training dataset\n",
    "    eval_dataset=tokenized_val,    # your pre-tokenized validation dataset\n",
    "    tokenizer=tokenizer,           # the tokenizer you used for preprocessing\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4190e55c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
