{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db7a04e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff8417f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "import torch\n",
    "from src.textSummarizer.utils.common import create_directories\n",
    "from typing import Dict, Tuple\n",
    "import evaluate \n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79c1be66",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_path: Path\n",
    "    tokenizer_path: Path\n",
    "    metric_file_name: str\n",
    "    batch_size: int = 8\n",
    "    max_length: int = 128\n",
    "    num_beams: int = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1924465",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        logger.info(f\"Evaluation will run on: {self.device}\")\n",
    "\n",
    "    def load_components(self) -> Tuple[BartForConditionalGeneration, BartTokenizer]:\n",
    "        \"\"\"Load model and tokenizer\"\"\"\n",
    "        try:\n",
    "            model = BartForConditionalGeneration.from_pretrained(str(self.config.model_path))\n",
    "            model = model.to(self.device)\n",
    "            \n",
    "            tokenizer = BartTokenizer.from_pretrained(str(self.config.tokenizer_path))\n",
    "            \n",
    "            return model, tokenizer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading components: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def load_datasets(self):\n",
    "        \"\"\"Load the test dataset for evaluation\"\"\"\n",
    "        try:\n",
    "            test_path = Path(self.config.data_path) / \"transformed_test_data\"\n",
    "            test_dataset = load_from_disk(test_path)\n",
    "            logger.info(f\"Loaded test dataset with {len(test_dataset)} samples\")\n",
    "            return test_dataset\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading datasets: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def compute_metrics(self, eval_pred, tokenizer) -> Dict[str, float]:\n",
    "        \"\"\"Compute evaluation metrics (ROUGE)\"\"\"\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "        predictions, labels = eval_pred\n",
    "\n",
    "        # Replace -100 in labels with pad_token_id for decoding\n",
    "        labels = [[(label if label != -100 else tokenizer.pad_token_id) for label in l] for l in labels]\n",
    "\n",
    "        # Decode predicted and reference summaries\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Clean text\n",
    "        decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "        decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "        # Compute ROUGE\n",
    "        result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "        result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "        return result\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Complete evaluation pipeline\"\"\"\n",
    "        try:\n",
    "            model, tokenizer = self.load_components()\n",
    "            test_dataset = self.load_datasets()\n",
    "            \n",
    "            # Prepare data collator\n",
    "            data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "            # Prepare the trainer\n",
    "            trainer = Seq2SeqTrainer(\n",
    "                model=model,\n",
    "                args=Seq2SeqTrainingArguments(\n",
    "                    output_dir=self.config.root_dir,\n",
    "                    per_device_eval_batch_size=self.config.batch_size,\n",
    "                    predict_with_generate=True,\n",
    "                    logging_dir=os.path.join(self.config.root_dir, \"logs\"),\n",
    "                ),\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=lambda eval_pred: self.compute_metrics(eval_pred, tokenizer),  # Pass tokenizer here\n",
    "            )\n",
    "\n",
    "            logger.info(\"Starting evaluation...\")\n",
    "            metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
    "            logger.info(f\"Evaluation metrics: {metrics}\")\n",
    "\n",
    "            # Save metrics\n",
    "            metrics_path = Path(self.config.root_dir) / self.config.metric_file_name\n",
    "            pd.DataFrame([metrics]).to_csv(metrics_path, index=False)\n",
    "            logger.info(f\"Metrics saved to {metrics_path}\")\n",
    "\n",
    "            return metrics\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Evaluation failed: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c2022a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-05 21:49:16,512: INFO: 1843591608: Evaluation will run on: cuda]\n",
      "[2025-07-05 21:49:17,142: INFO: 1843591608: Loaded test dataset with 150 samples]\n",
      "[2025-07-05 21:49:17,663: INFO: 1843591608: Starting evaluation...]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-05 21:50:06,196: INFO: rouge_scorer: Using default tokenizer.]\n",
      "[2025-07-05 21:50:06,582: INFO: 1843591608: Evaluation metrics: {'eval_loss': 1.1355241537094116, 'eval_model_preparation_time': 0.002, 'eval_rouge1': 25.1023, 'eval_rouge2': 11.8723, 'eval_rougeL': 20.1832, 'eval_rougeLsum': 23.3543, 'eval_runtime': 48.9063, 'eval_samples_per_second': 3.067, 'eval_steps_per_second': 0.388}]\n",
      "[2025-07-05 21:50:06,592: INFO: 1843591608: Metrics saved to artifacts\\model_evaluation\\evaluation_metrics.csv]\n",
      "ROUGE Scores: {'eval_loss': 1.1355241537094116, 'eval_model_preparation_time': 0.002, 'eval_rouge1': 25.1023, 'eval_rouge2': 11.8723, 'eval_rougeL': 20.1832, 'eval_rougeLsum': 23.3543, 'eval_runtime': 48.9063, 'eval_samples_per_second': 3.067, 'eval_steps_per_second': 0.388}\n"
     ]
    }
   ],
   "source": [
    "# Configure paths (should match your config.yaml)\n",
    "eval_config = ModelEvaluationConfig(\n",
    "    root_dir=Path(\"artifacts/model_evaluation\"),\n",
    "    data_path=Path(\"artifacts/data_transformation\"),\n",
    "    model_path=Path(\"artifacts/model_trainer/final_model\"),\n",
    "    tokenizer_path=Path(\"artifacts/model_trainer/final_model\"),\n",
    "    metric_file_name=\"evaluation_metrics.csv\"\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "evaluator = ModelEvaluation(eval_config)\n",
    "results = evaluator.evaluate()\n",
    "\n",
    "# Output results\n",
    "print(\"ROUGE Scores:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0ab917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b88e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b813b585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71e33a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Evaluation will run on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded test dataset with 150 samples\n",
      "INFO:__main__:Starting evaluation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Evaluation failed: name 'evaluate' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 196\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[0;32m    195\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m ModelEvaluation(eval_config, training_args_config)\n\u001b[1;32m--> 196\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Output results\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROUGE Scores:\u001b[39m\u001b[38;5;124m\"\u001b[39m, results)\n",
      "Cell \u001b[1;32mIn[2], line 170\u001b[0m, in \u001b[0;36mModelEvaluation.evaluate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    157\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m    158\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    159\u001b[0m     args\u001b[38;5;241m=\u001b[39mSeq2SeqTrainingArguments(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m eval_pred: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(eval_pred, tokenizer),  \u001b[38;5;66;03m# Pass tokenizer here\u001b[39;00m\n\u001b[0;32m    167\u001b[0m )\n\u001b[0;32m    169\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting evaluation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 170\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation metrics: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# Save metrics\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\anura\\miniconda3\\envs\\nlp\\lib\\site-packages\\transformers\\trainer_seq2seq.py:191\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[1;32m--> 191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\anura\\miniconda3\\envs\\nlp\\lib\\site-packages\\transformers\\trainer.py:4200\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4197\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   4199\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 4200\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4201\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4203\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   4204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   4205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4210\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   4211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Users\\anura\\miniconda3\\envs\\nlp\\lib\\site-packages\\transformers\\trainer.py:4490\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4488\u001b[0m     eval_set_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_losses \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4489\u001b[0m     eval_set_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_inputs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 4490\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meval_set_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4492\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4493\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4494\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[2], line 166\u001b[0m, in \u001b[0;36mModelEvaluation.evaluate.<locals>.<lambda>\u001b[1;34m(eval_pred)\u001b[0m\n\u001b[0;32m    154\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForSeq2Seq(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# Prepare the trainer using training arguments from the configuration\u001b[39;00m\n\u001b[0;32m    157\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m    158\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    159\u001b[0m     args\u001b[38;5;241m=\u001b[39mSeq2SeqTrainingArguments(\n\u001b[0;32m    160\u001b[0m         output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_args_config\u001b[38;5;241m.\u001b[39moutput_dir,\n\u001b[0;32m    161\u001b[0m         per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_args_config\u001b[38;5;241m.\u001b[39mper_device_eval_batch_size,\n\u001b[0;32m    162\u001b[0m         predict_with_generate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_args_config\u001b[38;5;241m.\u001b[39mpredict_with_generate,\n\u001b[0;32m    163\u001b[0m         logging_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_args_config\u001b[38;5;241m.\u001b[39mlogging_dir,\n\u001b[0;32m    164\u001b[0m     ),\n\u001b[0;32m    165\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m--> 166\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m eval_pred: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m,  \u001b[38;5;66;03m# Pass tokenizer here\u001b[39;00m\n\u001b[0;32m    167\u001b[0m )\n\u001b[0;32m    169\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting evaluation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    170\u001b[0m metrics \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset\u001b[38;5;241m=\u001b[39mtest_dataset)\n",
      "Cell \u001b[1;32mIn[2], line 127\u001b[0m, in \u001b[0;36mModelEvaluation.compute_metrics\u001b[1;34m(self, eval_pred, tokenizer)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_metrics\u001b[39m(\u001b[38;5;28mself\u001b[39m, eval_pred, tokenizer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute evaluation metrics (ROUGE)\"\"\"\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m     rouge \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrouge\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    128\u001b[0m     predictions, labels \u001b[38;5;241m=\u001b[39m eval_pred\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;66;03m# Replace -100 in labels with pad_token_id for decoding\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml\n",
    "from src.textSummarizer.constants import *\n",
    "from typing import Dict, Tuple\n",
    "import evaluate\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_path: Path\n",
    "    tokenizer_path: Path\n",
    "    metric_file_name: str\n",
    "\n",
    "@dataclass\n",
    "class Seq2SeqTrainingArgumentsConfig:\n",
    "    output_dir: Path\n",
    "    eval_strategy: str\n",
    "    eval_steps: int\n",
    "    logging_steps: int\n",
    "    save_steps: int\n",
    "    per_device_train_batch_size: int\n",
    "    per_device_eval_batch_size: int\n",
    "    gradient_accumulation_steps: int\n",
    "    num_train_epochs: int\n",
    "    save_total_limit: int\n",
    "    predict_with_generate: bool\n",
    "    fp16: bool\n",
    "    logging_dir: Path\n",
    "    report_to: str\n",
    "    learning_rate: float\n",
    "    weight_decay: float\n",
    "    warmup_steps: int\n",
    "    gradient_checkpointing: bool\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self, config_path: str=CONFIG_FILE_PATH, params_path: str=PARAMS_FILE_PATH):\n",
    "        self.config_path = config_path\n",
    "        self.params_path = params_path\n",
    "        self.model_evaluation_config = self.load_model_evaluation_config()\n",
    "        self.training_args_config = self.load_training_args_config()\n",
    "\n",
    "    def load_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        \"\"\"Load model evaluation configuration from YAML file.\"\"\"\n",
    "        with open(self.config_path, 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "            model_eval_config = config['model_evaluation']\n",
    "            return ModelEvaluationConfig(\n",
    "                root_dir=Path(model_eval_config['root_dir']),\n",
    "                data_path=Path(model_eval_config['data_path']),\n",
    "                model_path=Path(model_eval_config['model_path']),\n",
    "                tokenizer_path=Path(model_eval_config['tokenizer_path']),\n",
    "                metric_file_name=model_eval_config['metric_file_name']\n",
    "            )\n",
    "\n",
    "    def load_training_args_config(self) -> Seq2SeqTrainingArgumentsConfig:\n",
    "        \"\"\"Load training arguments configuration from YAML file.\"\"\"\n",
    "        with open(self.params_path, 'r') as file:\n",
    "            params = yaml.safe_load(file)\n",
    "            training_args = params['Seq2SeqTrainingArguments']\n",
    "            return Seq2SeqTrainingArgumentsConfig(\n",
    "                output_dir=Path(training_args['output_dir']),\n",
    "                eval_strategy=training_args['eval_strategy'],\n",
    "                eval_steps=training_args['eval_steps'],\n",
    "                logging_steps=training_args['logging_steps'],\n",
    "                save_steps=training_args['save_steps'],\n",
    "                per_device_train_batch_size=training_args['per_device_train_batch_size'],\n",
    "                per_device_eval_batch_size=training_args['per_device_eval_batch_size'],\n",
    "                gradient_accumulation_steps=training_args['gradient_accumulation_steps'],\n",
    "                num_train_epochs=training_args['num_train_epochs'],\n",
    "                save_total_limit=training_args['save_total_limit'],\n",
    "                predict_with_generate=training_args['predict_with_generate'],\n",
    "                fp16=training_args['fp16'],\n",
    "                logging_dir=Path(training_args['logging_dir']),\n",
    "                report_to=training_args['report_to'],\n",
    "                learning_rate=training_args['learning_rate'],\n",
    "                weight_decay=training_args['weight_decay'],\n",
    "                warmup_steps=training_args['warmup_steps'],\n",
    "                gradient_checkpointing=training_args['gradient_checkpointing']\n",
    "            )\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig, training_args_config: Seq2SeqTrainingArgumentsConfig):\n",
    "        self.config = config\n",
    "        self.training_args_config = training_args_config  # Store training args config\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        logger.info(f\"Evaluation will run on: {self.device}\")\n",
    "\n",
    "    def load_components(self) -> Tuple[BartForConditionalGeneration, BartTokenizer]:\n",
    "        \"\"\"Load model and tokenizer\"\"\"\n",
    "        try:\n",
    "            model = BartForConditionalGeneration.from_pretrained(str(self.config.model_path))\n",
    "            model = model.to(self.device)\n",
    "            \n",
    "            tokenizer = BartTokenizer.from_pretrained(str(self.config.tokenizer_path))\n",
    "            \n",
    "            return model, tokenizer\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading components: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def load_datasets(self):\n",
    "        \"\"\"Load the test dataset for evaluation\"\"\"\n",
    "        try:\n",
    "            test_path = Path(self.config.data_path) / \"transformed_test_data\"\n",
    "            test_dataset = load_from_disk(test_path)\n",
    "            logger.info(f\"Loaded test dataset with {len(test_dataset)} samples\")\n",
    "            return test_dataset\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading datasets: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def compute_metrics(self, eval_pred, tokenizer) -> Dict[str, float]:\n",
    "        \"\"\"Compute evaluation metrics (ROUGE)\"\"\"\n",
    "        rouge = evaluate.load(\"rouge\")\n",
    "        predictions, labels = eval_pred\n",
    "\n",
    "        # Replace -100 in labels with pad_token_id for decoding\n",
    "        labels = [[(label if label != -100 else tokenizer.pad_token_id) for label in l] for l in labels]\n",
    "\n",
    "        # Decode predicted and reference summaries\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Clean text\n",
    "        decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "        decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "        # Compute ROUGE\n",
    "        result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "        result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "        return result\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Complete evaluation pipeline\"\"\"\n",
    "        try:\n",
    "            model, tokenizer = self.load_components()\n",
    "            test_dataset = self.load_datasets()\n",
    "            \n",
    "            # Prepare data collator\n",
    "            data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "            # Prepare the trainer using training arguments from the configuration\n",
    "            trainer = Seq2SeqTrainer(\n",
    "                model=model,\n",
    "                args=Seq2SeqTrainingArguments(\n",
    "                    output_dir=self.training_args_config.output_dir,\n",
    "                    per_device_eval_batch_size=self.training_args_config.per_device_eval_batch_size,\n",
    "                    predict_with_generate=self.training_args_config.predict_with_generate,\n",
    "                    logging_dir=self.training_args_config.logging_dir,\n",
    "                ),\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=lambda eval_pred: self.compute_metrics(eval_pred, tokenizer),  # Pass tokenizer here\n",
    "            )\n",
    "\n",
    "            logger.info(\"Starting evaluation...\")\n",
    "            metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
    "            logger.info(f\"Evaluation metrics: {metrics}\")\n",
    "\n",
    "            # Save metrics\n",
    "            metrics_path = Path(self.config.root_dir) / self.config.metric_file_name\n",
    "            pd.DataFrame([metrics]).to_csv(metrics_path, index=False)\n",
    "            logger.info(f\"Metrics saved to {metrics_path}\")\n",
    "\n",
    "            return metrics\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Evaluation failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Load configurations\n",
    "    config_manager = ConfigurationManager()\n",
    "\n",
    "    # Access model evaluation config\n",
    "    eval_config = config_manager.model_evaluation_config\n",
    "\n",
    "    # Access training arguments config\n",
    "    training_args_config = config_manager.training_args_config\n",
    "\n",
    "    # Run evaluation\n",
    "    evaluator = ModelEvaluation(eval_config, training_args_config)\n",
    "    results = evaluator.evaluate()\n",
    "\n",
    "    # Output results\n",
    "    print(\"ROUGE Scores:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fa5a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
