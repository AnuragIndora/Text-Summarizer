{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anura\\miniconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "import evaluate\n",
    "import torch\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Load & Subsample CNN/DailyMail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsample to reduce training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    BartTokenizer,\n",
    "    BartForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "import evaluate\n",
    "from random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(42)\n",
    "train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(50000))\n",
    "val_dataset = dataset[\"validation\"].shuffle(seed=42).select(range(3000))\n",
    "test_dataset = dataset[\"test\"].shuffle(seed=42).select(range(3000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Load Tokenizer & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/bart-base\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(\n",
    "    \"facebook/bart-base\",\n",
    "    gradient_checkpointing=True  # Saves memory during training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    inputs = tokenizer(\n",
    "        example[\"article\"], max_length=1024, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            example[\"highlights\"], max_length=128, truncation=True, padding=\"max_length\"\n",
    "        )\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_train = train_dataset.map(preprocess, batched=True, remove_columns=train_dataset.column_names)\n",
    "tokenized_val = val_dataset.map(preprocess, batched=True, remove_columns=val_dataset.column_names)\n",
    "tokenized_test = test_dataset.map(preprocess, batched=True, remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Define Evaluation Metric (ROUGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# âœ… Correct compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Replace -100 in labels with pad_token_id for decoding\n",
    "    labels = [[(label if label != -100 else tokenizer.pad_token_id) for label in l] for l in labels]\n",
    "\n",
    "    # Decode predicted and reference summaries\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Clean text\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    # Compute ROUGE\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    logging_steps=500,\n",
    "    save_steps=1000,\n",
    "    per_device_train_batch_size=2,        # Small batch for limited GPU\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,        # Effective larger batch\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,           # âœ… Needed for summarization\n",
    "    fp16=True,                            # Use if you have a compatible GPU\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",                     # Can be \"wandb\", \"tensorboard\", etc.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Define Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anura\\AppData\\Local\\Temp\\ipykernel_28820\\3214467047.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,                    # e.g. BartForConditionalGeneration or T5ForConditionalGeneration\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,             # instance of Seq2SeqTrainingArguments\n",
    "    train_dataset=tokenized_train, # your pre-tokenized training dataset\n",
    "    eval_dataset=tokenized_val,    # your pre-tokenized validation dataset\n",
    "    tokenizer=tokenizer,           # the tokenizer you used for preprocessing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18750' max='18750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18750/18750 5:31:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.607700</td>\n",
       "      <td>1.104044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.155900</td>\n",
       "      <td>1.104603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.124000</td>\n",
       "      <td>1.072914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.100700</td>\n",
       "      <td>1.048071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.098200</td>\n",
       "      <td>1.050083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.097000</td>\n",
       "      <td>1.041155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.081000</td>\n",
       "      <td>1.056923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.071700</td>\n",
       "      <td>1.028411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.079200</td>\n",
       "      <td>1.029974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.057000</td>\n",
       "      <td>1.025284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.038400</td>\n",
       "      <td>1.013633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.048500</td>\n",
       "      <td>1.006771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.002300</td>\n",
       "      <td>1.016639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.947400</td>\n",
       "      <td>1.006566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.939700</td>\n",
       "      <td>1.010302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.932700</td>\n",
       "      <td>1.008492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.938600</td>\n",
       "      <td>1.000403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.946800</td>\n",
       "      <td>0.995652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.941800</td>\n",
       "      <td>0.993827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.929900</td>\n",
       "      <td>1.001353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.922200</td>\n",
       "      <td>0.988805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.933400</td>\n",
       "      <td>0.996058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.926200</td>\n",
       "      <td>0.989716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.921000</td>\n",
       "      <td>0.986178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.921800</td>\n",
       "      <td>0.985526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.849900</td>\n",
       "      <td>0.991380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.844900</td>\n",
       "      <td>0.989297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.847000</td>\n",
       "      <td>0.985466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.856000</td>\n",
       "      <td>0.984061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.851200</td>\n",
       "      <td>0.983802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.845800</td>\n",
       "      <td>0.977762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.846200</td>\n",
       "      <td>0.976264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.842800</td>\n",
       "      <td>0.977728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>0.976895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.847700</td>\n",
       "      <td>0.974140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.846900</td>\n",
       "      <td>0.975352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.836100</td>\n",
       "      <td>0.974883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anura\\miniconda3\\envs\\nlp\\lib\\site-packages\\transformers\\modeling_utils.py:3685: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18750, training_loss=0.9688578588867187, metrics={'train_runtime': 19882.3783, 'train_samples_per_second': 7.544, 'train_steps_per_second': 0.943, 'total_flos': 9.1460468736e+16, 'train_loss': 0.9688578588867187, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('.\\\\tokenizer_config.json',\n",
       " '.\\\\special_tokens_map.json',\n",
       " '.\\\\vocab.json',\n",
       " '.\\\\merges.txt',\n",
       " '.\\\\added_tokens.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\".\")\n",
    "tokenizer.save_pretrained(\".\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Inference on Custom Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anura\\miniconda3\\envs\\nlp\\lib\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Summary:\n",
      " The European Central Bank has decided to leave interest rates unchanged as the economic outlook remains uncertain .\n"
     ]
    }
   ],
   "source": [
    "article = \"The European Central Bank has decided to leave interest rates unchanged as the economic outlook remains uncertain...\"\n",
    "\n",
    "inputs = tokenizer(article, return_tensors=\"pt\", truncation=True, max_length=1024).to(model.device)\n",
    "summary_ids = model.generate(inputs[\"input_ids\"], max_length=128, num_beams=4, early_stopping=True)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nGenerated Summary:\\n\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Tokenize the test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test = test_dataset.map(preprocess, batched=True, remove_columns=dataset[\"test\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Run Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 14:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Test Set ROUGE Metrics: {'eval_loss': 0.9473769664764404, 'eval_runtime': 71.0281, 'eval_samples_per_second': 42.237, 'eval_steps_per_second': 21.118, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(eval_dataset=tokenized_test)\n",
    "print(\"ðŸ“Š Test Set ROUGE Metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Test Set ROUGE Metrics: {'eval_loss': 0.9473769664764404, 'eval_rouge1': 25.2112, 'eval_rouge2': 12.28, 'eval_rougeL': 20.6701, 'eval_rougeLsum': 23.6094, 'eval_runtime': 813.7209, 'eval_samples_per_second': 3.687, 'eval_steps_per_second': 1.843, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# âœ… Reattach compute_metrics to Trainer (optional if already done)\n",
    "trainer.compute_metrics = compute_metrics\n",
    "\n",
    "# âœ… Proper evaluation â€” this will generate predictions and compute ROUGE\n",
    "metrics = trainer.evaluate(eval_dataset=tokenized_test)\n",
    "print(\"ðŸ“Š Test Set ROUGE Metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Summaries for Test Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“° Article #1\n",
      "ðŸ“Œ Reference Summary: CNN's Dr. Sanjay Gupta says we should legalize medical marijuana now .\n",
      "He says he knows how easy it is do nothing \"because I did nothing for too long\"\n",
      "ðŸ§  Generated Summary: For the first time a majority, 53%, favor marijuana legalization .\n",
      "Support for legalization has risen 11 points in the past few years alone .\n",
      "\"Weed\" is the first federally approved clinical study on the use of marijuana for PTSD .\n",
      "\n",
      "ðŸ“° Article #2\n",
      "ðŸ“Œ Reference Summary: Child has amassed thousands of Twitter followers with 'gang life' photos .\n",
      "In one video he points gun at camera as adults look on unfazed .\n",
      "His tweets have prompted backlash with calls for intervention .\n",
      "ðŸ§  Generated Summary: Baby-faced boy from Memphis, Tennessee, poses with guns, cash, and bags of marijuana .\n",
      "Tweets include phrases such as 'I need a bad b****', 'f*** da police', and 'gang sh** n****'\n",
      "As he is a minor, DailyMail.com will not identify the boy .\n",
      "\n",
      "ðŸ“° Article #3\n",
      "ðŸ“Œ Reference Summary: The presidential hopeful held a town hall meeting in Kenilworth on Tuesday .\n",
      "During the meeting, high school English teacher Kathy Mooney got up to ask the governor a question about pensions .\n",
      "She asked why he didn't seek a higher legal settlement in a case with ExxonMobil that would have contributed to the state's pension system .\n",
      "Christie responded by repeatedly asking how much Mooney knew about the deal instead of answering her question .\n",
      "ðŸ§  Generated Summary: New Jersey Governor Chris Christie got into a heated debate with a veteran teacher at a town hall meeting Tuesday night .\n",
      "The state's largest teacher's union is calling him out for his 'bullying' behavior .\n",
      "'He's always taken a very nasty and disrespectful tone with teachers and other individuals who dare to question him at these events,' Steve Wollmer of the NJ Education Association told NJ.com .\n",
      "\n",
      "ðŸ“° Article #4\n",
      "ðŸ“Œ Reference Summary: Cassey Ho boasts over two million subscribers on her YouTube channel Blogilates .\n",
      "The 28-year-old receives hundreds of comments a day telling her that she needs to lose weight .\n",
      "ðŸ§  Generated Summary: Crissey Ho's YouTube channel, Blogilates, has over two million subscribers .\n",
      "But the negative comments left under the California residentâ€™s fitness videos still manage to get under her skin .\n",
      "In her new video, the 28-year-old says even she has felt bad about herself after reading nasty comments calling her 'fat' online .\n",
      "\n",
      "ðŸ“° Article #5\n",
      "ðŸ“Œ Reference Summary: Aaron Cook was overlooked by Team GB for the London Olympics .\n",
      "Taekwondo star has received citizenship from Moldova and plans to fight for them at the Rio 2016 Games .\n",
      "The British Olympic Association could yet block the move .\n",
      "ðŸ§  Generated Summary: Dorset-born Cook, 24, applied for citizenship after receiving funding from Moldovan billionaire Igor Iuzefovici .\n",
      "He has now received his passport from the small eastern European state .\n",
      "'I will soon begin a new journey representing the Republic of Moldova at all International competitions and hopefully the Rio Olympic games and beyond,' Cook wrote on his Facebook page .\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    article = test_dataset[i][\"article\"]\n",
    "    reference = test_dataset[i][\"highlights\"]\n",
    "\n",
    "    inputs = tokenizer(article, return_tensors=\"pt\", truncation=True, max_length=1024).to(model.device)\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], max_length=128, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"\\nðŸ“° Article #{i+1}\")\n",
    "    print(\"ðŸ“Œ Reference Summary:\", reference)\n",
    "    print(\"ðŸ§  Generated Summary:\", summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
