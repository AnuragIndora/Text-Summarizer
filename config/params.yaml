Seq2SeqTrainingArguments:
  output_dir: "./results"
  eval_strategy: "steps"
  eval_steps: 500
  logging_steps: 500
  save_steps: 1000
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 4
  num_train_epochs: 3
  save_total_limit: 2
  predict_with_generate: true
  fp16: true
  logging_dir: "./logs"
  report_to: "none"
  learning_rate: 5e-5
  weight_decay: 0.01
  warmup_steps: 500
  gradient_checkpointing: True
